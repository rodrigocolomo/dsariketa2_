{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similitud de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip3 install nltk\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language toolkit (NLTK) es la biblioteca más popular para el procesamiento de lenguaje natural (NLP) en Python. Contiene librerías de procesamiento de texto para tokenización, análisis sintáctico, clasificación, derivación, etiquetado y razonamiento semántico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install gensim\n",
    "\n",
    "#pip install --user gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim es una librería Procesamiento de Lenguaje Natural (NLP) para procesar textos, trabajando con modelos de vectores de palabras (como Word2Vec, FastText, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\colom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\colom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"La Inteligencia Aritifical son sistemas que permiten dotar a las máquinas de inteligencia para imitar destrezas humanas.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos el método word_tokenize() para dividir una frase en palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sent_tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "file_docs = []\n",
    "\n",
    "with open ('demo.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file_docs.append(line)\n",
    "\n",
    "print(\"Number of documents:\",len(file_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos el archivo demo.txt y agregamos frases con token en la matriz para la conversión de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in file_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un diccionario mapea a cada palabra con un número. Gensim te permite leer el texto y actualizar el diccionario, una línea a la vez, sin cargar todo el archivo de texto en la memoria del sistema.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "for doc in tf_idf[corpus]:\n",
    "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = gensim.similarities.Similarity('a/',tf_idf[corpus],\n",
    "                                        num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2_docs = []\n",
    "\n",
    "with open ('demo2.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file2_docs.append(line)\n",
    "\n",
    "print(\"Number of documents:\",len(file2_docs))  \n",
    "for line in file2_docs:\n",
    "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_doc_tf_idf = tf_idf[query_doc_bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Comparing Result:', sims[query_doc_tf_idf]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "print(sum_of_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_of_similarity = round(float((sum_of_sims / len(file_docs)) * 100))\n",
    "print(f'Average similarity float: {float(sum_of_sims / len(file_docs))}')\n",
    "print(f'Average similarity percentage: {float(sum_of_sims / len(file_docs)) * 100}')\n",
    "print(f'Average similarity rounded percentage: {percentage_of_similarity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
